from __future__ import annotations

import json
import time
import uuid
from typing import Any, AsyncGenerator, Dict, Optional, Set

import httpx
from .logging import logger
from .token_counter import count_packet_tokens, count_tokens

from .config import BRIDGE_BASE_URL
from .helpers import _get


# è‡ªå®šä¹‰å¼‚å¸¸ï¼šç”¨äºè§¦å‘è‡ªåŠ¨æ¢å¤
class InternalErrorRecoverable(Exception):
    """è¡¨ç¤ºé‡åˆ°äº†å¯æ¢å¤çš„ internal_error"""
    def __init__(self, tool_name: Optional[str], error_message: str):
        self.tool_name = tool_name
        self.error_message = error_message
        super().__init__(f"Internal error with tool: {tool_name}")


class LLMUnavailableRecoverable(Exception):
    """è¡¨ç¤ºé‡åˆ°äº†å¯æ¢å¤çš„ llm_unavailable"""
    pass


def get_model_context_window(model_name: str) -> int:
    """æ ¹æ®æ¨¡å‹åç§°è·å–ä¸Šä¸‹æ–‡çª—å£å¤§å°

    åŸºäºå·²çŸ¥çš„ Claude æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£ï¼š
    - Claude 3.5 Sonnet: 200k tokens
    - Claude 3 Opus: 200k tokens
    - Claude 3.5 Haiku: 200k tokens
    - Claude 4 Sonnet: 200k tokens (å‡è®¾)
    - Claude 4.1 Opus: 200k tokens (å‡è®¾)
    - é»˜è®¤: 100k tokens (ä¿å®ˆä¼°è®¡)
    """
    model_lower = model_name.lower() if model_name else ""

    # Claude 3 å’Œ 3.5 ç³»åˆ—
    if "claude-3" in model_lower:
        return 200000

    # Claude 4 ç³»åˆ— (åŒ…æ‹¬ claude-4-sonnet å’Œ claude-4.1-opus)
    if "claude-4" in model_lower:
        # claude-4.1-opus ä¹ŸåŒ…å«åœ¨è¿™é‡Œ
        return 200000

    # é»˜è®¤å€¼ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
    return 100000


async def _process_sse_response_lines(response, completion_id: str, created_ts: int, model_id: str, input_tokens: int = 0) -> AsyncGenerator[str, None]:
    """å¤„ç† SSE å“åº”æµçš„å…±ç”¨å‡½æ•°

    Args:
        response: HTTP å“åº”å¯¹è±¡
        completion_id: å®Œæˆè¯·æ±‚ ID
        created_ts: åˆ›å»ºæ—¶é—´æˆ³
        model_id: æ¨¡å‹ ID
        input_tokens: é¢„è®¡ç®—çš„è¾“å…¥ token æ•°
    """
    current = ""
    tool_calls_emitted = False
    output_text = ""  # ç´¯ç§¯æ‰€æœ‰è¾“å‡ºæ–‡æœ¬ï¼Œç”¨äºå‡†ç¡®è®¡ç®— token
    async for line in response.aiter_lines():
        if line.startswith("data:"):
            payload = line[5:].strip()
            if not payload:
                continue
            # æ‰“å°æ¥æ”¶åˆ°çš„ Protobuf SSE åŸå§‹äº‹ä»¶ç‰‡æ®µ
            try:
                logger.info("[OpenAI Compat] æ¥æ”¶åˆ°çš„ Protobuf SSE(data): %s", payload)
            except Exception:
                pass
            if payload == "[DONE]":
                break
            current += payload
            continue
        if (line.strip() == "") and current:
            try:
                ev = json.loads(current)
            except Exception:
                current = ""
                continue
            current = ""
            event_data = (ev or {}).get("parsed_data") or {}

            # æ‰“å°æ¥æ”¶åˆ°çš„ Protobuf äº‹ä»¶ï¼ˆè§£æåï¼‰
            try:
                logger.info("[OpenAI Compat] æ¥æ”¶åˆ°çš„ Protobuf äº‹ä»¶(parsed): %s", json.dumps(event_data, ensure_ascii=False))
            except Exception:
                pass

            if "init" in event_data:
                pass

            client_actions = _get(event_data, "client_actions", "clientActions")
            if isinstance(client_actions, dict):
                actions = _get(client_actions, "actions", "Actions") or []
                for action in actions:
                    append_data = _get(action, "append_to_message_content", "appendToMessageContent")
                    if isinstance(append_data, dict):
                        message = append_data.get("message", {})
                        agent_output = _get(message, "agent_output", "agentOutput") or {}
                        text_content = agent_output.get("text", "")
                        if text_content:
                            output_text += text_content  # ç´¯ç§¯è¾“å‡ºæ–‡æœ¬
                            delta = {
                                "id": completion_id,
                                "object": "chat.completion.chunk",
                                "created": created_ts,
                                "model": model_id,
                                "choices": [{"index": 0, "delta": {"content": text_content}}],
                            }
                            # æ‰“å°è½¬æ¢åçš„ OpenAI SSE äº‹ä»¶
                            try:
                                logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit): %s", json.dumps(delta, ensure_ascii=False))
                            except Exception:
                                pass
                            yield f"data: {json.dumps(delta, ensure_ascii=False)}\n\n"

                    messages_data = _get(action, "add_messages_to_task", "addMessagesToTask")
                    if isinstance(messages_data, dict):
                        messages = messages_data.get("messages", [])
                        for message in messages:
                            # å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·è°ƒç”¨ç»“æœ
                            tool_call_result = _get(message, "tool_call_result", "toolCallResult") or {}
                            if tool_call_result:
                                # è¿™æ˜¯å·¥å…·è°ƒç”¨ç»“æœï¼ŒæŒ‰åŸæœ‰é€»è¾‘å¤„ç†
                                if isinstance(tool_call_result, dict) and tool_call_result.get("tool_call_id"):
                                    tool_call_id = tool_call_result.get("tool_call_id")
                                    server_result = _get(tool_call_result, "server", "server") or {}
                                    serialized_result = server_result.get("serialized_result", "")

                                    # è§£ç  serialized_result (Base64URL)
                                    result_content = ""
                                    task_data = None
                                    if serialized_result:
                                        try:
                                            import base64
                                            # Base64URL è§£ç 
                                            decoded_bytes = base64.urlsafe_b64decode(serialized_result + '=' * (-len(serialized_result) % 4))

                                            # å°è¯•ç”¨ blackboxprotobuf è§£æä¸º protobuf
                                            try:
                                                import blackboxprotobuf
                                            except ImportError:
                                                blackboxprotobuf = None

                                            if blackboxprotobuf:
                                                try:
                                                    decoded_data, _ = blackboxprotobuf.decode_message(decoded_bytes)
                                                    logger.info("[OpenAI Compat] tool_call_result serialized_result è§£ç æˆåŠŸ: %s", json.dumps(decoded_data, ensure_ascii=False))

                                                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»åŠ¡æ•°æ® (11 æˆ– 9 é”®)
                                                    if "11" in decoded_data or "9" in decoded_data:
                                                        task_data = decoded_data
                                                        logger.info("[OpenAI Compat] æ£€æµ‹åˆ° tool_call_result ä¸­çš„ä»»åŠ¡æ•°æ®")
                                                    else:
                                                        # ä¸æ˜¯ä»»åŠ¡æ•°æ®ï¼ŒæŒ‰åŸæ¥çš„æ–¹å¼å¤„ç†
                                                        result_content = decoded_bytes.decode('utf-8')
                                                except Exception as e:
                                                    logger.debug("[OpenAI Compat] tool_call_result Protobuf è§£ç å¤±è´¥: %s", e)
                                                    result_content = decoded_bytes.decode('utf-8')
                                            else:
                                                result_content = decoded_bytes.decode('utf-8')

                                            if not task_data:
                                                logger.info("[OpenAI Compat] è§£ç å·¥å…·ç»“æœ: %s", result_content[:200] + "..." if len(result_content) > 200 else result_content)
                                        except Exception as e:
                                            logger.error("[OpenAI Compat] è§£ç  serialized_result å¤±è´¥: %s", e)
                                            result_content = f"[è§£ç å¤±è´¥: {str(e)}]"

                                    # å¦‚æœæ£€æµ‹åˆ°ä»»åŠ¡æ•°æ®ï¼Œç”Ÿæˆ TodoWrite å·¥å…·è°ƒç”¨
                                    if task_data:
                                        # è½¬æ¢ä»»åŠ¡æ•°æ®ä¸º TodoWrite æ ¼å¼
                                        todos = []
                                        task_container = None

                                        # æ£€æŸ¥æ˜¯å¦æ˜¯ä»»åŠ¡åˆ—è¡¨æ•°æ® (11 æˆ– 9 é”®)
                                        if "11" in task_data:
                                            nested_data = task_data["11"]
                                            if isinstance(nested_data, dict) and "1" in nested_data:
                                                task_container = nested_data["1"]
                                        elif "9" in task_data:
                                            nested_data = task_data["9"]
                                            if isinstance(nested_data, dict) and "1" in nested_data:
                                                task_container = nested_data["1"]

                                        if task_container and isinstance(task_container, dict):
                                            # æœªå¼€å§‹ä»»åŠ¡
                                            if "1" in task_container and isinstance(task_container["1"], list):
                                                for task in task_container["1"]:
                                                    if isinstance(task, dict) and "1" in task:
                                                        todos.append({
                                                            "content": task.get("2", ""),
                                                            "status": "pending",
                                                            "activeForm": f"æ‰§è¡Œ {task.get('2', '')}"
                                                        })

                                            # å·²å®Œæˆä»»åŠ¡
                                            if "2" in task_container and isinstance(task_container["2"], list):
                                                for task in task_container["2"]:
                                                    if isinstance(task, dict) and "1" in task:
                                                        todos.append({
                                                            "content": task.get("2", ""),
                                                            "status": "completed",
                                                            "activeForm": f"å·²å®Œæˆ {task.get('2', '')}"
                                                        })

                                        # ç”Ÿæˆ TodoWrite å·¥å…·è°ƒç”¨
                                        todo_args = json.dumps({"todos": todos}, ensure_ascii=False)
                                        delta = {
                                            "id": completion_id,
                                            "object": "chat.completion.chunk",
                                            "created": created_ts,
                                            "model": model_id,
                                            "choices": [{
                                                "index": 0,
                                                "delta": {
                                                    "tool_calls": [{
                                                        "index": 0,
                                                        "id": tool_call_id,
                                                        "type": "function",
                                                        "function": {"name": "TodoWrite", "arguments": todo_args},
                                                    }]
                                                }
                                            }],
                                        }
                                        logger.info("[OpenAI Compat] ç”Ÿæˆ TodoWrite å·¥å…·è°ƒç”¨ï¼ŒåŒ…å« %d ä¸ªä»»åŠ¡", len(todos))
                                    # else:
                                    #     # å‘é€æ™®é€šå·¥å…·è°ƒç”¨ç»“æœ
                                    #     delta = {
                                    #         "id": completion_id,
                                    #         "object": "chat.completion.chunk",
                                    #         "created": created_ts,
                                    #         "model": model_id,
                                    #         "choices": [{
                                    #             "index": 0,
                                    #             "delta": {
                                    #                 "tool_calls": [{
                                    #                     "index": 0,
                                    #                     "id": tool_call_id,
                                    #                     "type": "function",
                                    #                     "function": {"name": "tool_result", "arguments": "{}"},
                                    #                 }]
                                    #             }
                                    #         }],
                                    #     }
                                        try:
                                            logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit tool_result): %s", json.dumps(delta, ensure_ascii=False))
                                        except Exception:
                                            pass
                                        yield f"data: {json.dumps(delta, ensure_ascii=False)}\n\n"

                                    # å‘é€å·¥å…·ç»“æœå†…å®¹ï¼ˆä»…å½“ä¸æ˜¯ä»»åŠ¡æ•°æ®æ—¶ï¼‰
                                    # if result_content and not task_data:
                                    #     content_delta = {
                                    #         "id": completion_id,
                                    #         "object": "chat.completion.chunk",
                                    #         "created": created_ts,
                                    #         "model": model_id,
                                    #         "choices": [{
                                    #             "index": 0,
                                    #             "delta": {
                                    #                 "tool_calls": [{
                                    #                     "index": 0,
                                    #                     "id": tool_call_id,
                                    #                     "type": "function",
                                    #                     "function": {"name": "tool_result_content", "arguments": json.dumps({"content": result_content}, ensure_ascii=False)},
                                    #                 }]
                                    #             }
                                    #         }],
                                    #     }
                                    #     try:
                                    #         logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit tool_result_content): %s", json.dumps(content_delta, ensure_ascii=False))
                                    #     except Exception:
                                    #         pass
                                    #     yield f"data: {json.dumps(content_delta, ensure_ascii=False)}\n\n"
                            else:
                                # å¤„ç†å·¥å…·è°ƒç”¨
                                tool_call = _get(message, "tool_call", "toolCall") or {}
                                call_mcp = _get(tool_call, "call_mcp_tool", "callMcpTool") or {}
                                if isinstance(call_mcp, dict) and call_mcp.get("name"):
                                    try:
                                        args_obj = call_mcp.get("args", {}) or {}
                                        args_str = json.dumps(args_obj, ensure_ascii=False)
                                    except Exception:
                                        args_str = "{}"
                                    tool_call_id = tool_call.get("tool_call_id") or str(uuid.uuid4())
                                    delta = {
                                        "id": completion_id,
                                        "object": "chat.completion.chunk",
                                        "created": created_ts,
                                        "model": model_id,
                                        "choices": [{
                                            "index": 0,
                                            "delta": {
                                                "tool_calls": [{
                                                    "index": 0,
                                                    "id": tool_call_id,
                                                    "type": "function",
                                                    "function": {"name": call_mcp.get("name"), "arguments": args_str},
                                                }]
                                            }
                                        }],
                                    }
                                    # æ‰“å°è½¬æ¢åçš„ OpenAI å·¥å…·è°ƒç”¨äº‹ä»¶
                                    try:
                                        logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit tool_calls): %s", json.dumps(delta, ensure_ascii=False))
                                    except Exception:
                                        pass
                                    yield f"data: {json.dumps(delta, ensure_ascii=False)}\n\n"
                                    tool_calls_emitted = True
                                else:
                                    # å¤„ç†æ™®é€šæ–‡æœ¬å†…å®¹
                                    agent_output = _get(message, "agent_output", "agentOutput") or {}
                                    text_content = agent_output.get("text", "")
                                    if text_content:
                                        output_text += text_content  # ç´¯ç§¯è¾“å‡ºæ–‡æœ¬
                                        delta = {
                                            "id": completion_id,
                                            "object": "chat.completion.chunk",
                                            "created": created_ts,
                                            "model": model_id,
                                            "choices": [{"index": 0, "delta": {"content": text_content}}],
                                        }
                                        try:
                                            logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit): %s", json.dumps(delta, ensure_ascii=False))
                                        except Exception:
                                            pass
                                        yield f"data: {json.dumps(delta, ensure_ascii=False)}\n\n"

                    # å¤„ç† update_task_message
                    update_task_message = _get(action, "update_task_message", "updateTaskMessage")
                    if isinstance(update_task_message, dict):
                        message = update_task_message.get("message", {})
                        if isinstance(message, dict):
                            # å¤„ç† agent_output.text
                            agent_output = _get(message, "agent_output", "agentOutput") or {}
                            text_content = agent_output.get("text", "")
                            if text_content:
                                output_text += text_content  # ç´¯ç§¯è¾“å‡ºæ–‡æœ¬
                                delta = {
                                    "id": completion_id,
                                    "object": "chat.completion.chunk",
                                    "created": created_ts,
                                    "model": model_id,
                                    "choices": [{"index": 0, "delta": {"content": text_content}}],
                                }
                                try:
                                    logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit update_task_message): %s", json.dumps(delta, ensure_ascii=False))
                                except Exception:
                                    pass
                                yield f"data: {json.dumps(delta, ensure_ascii=False)}\n\n"

                    # å¤„ç† create_task
                    create_task = _get(action, "create_task", "createTask")
                    if isinstance(create_task, dict):
                        task = create_task.get("task", {})
                        if isinstance(task, dict):
                            messages = task.get("messages", [])
                            for message in messages:
                                if isinstance(message, dict):
                                    agent_output = _get(message, "agent_output", "agentOutput") or {}
                                    text_content = agent_output.get("text", "")
                                    if text_content:
                                        output_text += text_content  # ç´¯ç§¯è¾“å‡ºæ–‡æœ¬
                                        delta = {
                                            "id": completion_id,
                                            "object": "chat.completion.chunk",
                                            "created": created_ts,
                                            "model": model_id,
                                            "choices": [{"index": 0, "delta": {"content": text_content}}],
                                        }
                                        try:
                                            logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit create_task): %s", json.dumps(delta, ensure_ascii=False))
                                        except Exception:
                                            pass
                                        yield f"data: {json.dumps(delta, ensure_ascii=False)}\n\n"

                    # å¤„ç† update_task_summary
                    update_task_summary = _get(action, "update_task_summary", "updateTaskSummary")
                    if isinstance(update_task_summary, dict):
                        summary = update_task_summary.get("summary", "")
                        if summary:
                            output_text += summary  # ç´¯ç§¯è¾“å‡ºæ–‡æœ¬
                            delta = {
                                "id": completion_id,
                                "object": "chat.completion.chunk",
                                "created": created_ts,
                                "model": model_id,
                                "choices": [{"index": 0, "delta": {"content": summary}}],
                            }
                            try:
                                logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit update_task_summary): %s", json.dumps(delta, ensure_ascii=False))
                            except Exception:
                                pass
                            yield f"data: {json.dumps(delta, ensure_ascii=False)}\n\n"

                    # å¤„ç† add_messages_to_task ä¸­çš„ tool_call server.payload (Base64ç¼–ç çš„ä»»åŠ¡åˆå§‹åŒ–æ•°æ®)
                    # å¤„ç†æ›´æ–°ä»»åŠ¡æ¶ˆæ¯äº‹ä»¶ç­‰å…¶ä»–é€»è¾‘...
                    # (ä¸ºäº†ç®€åŒ–æå–ï¼Œè¿™é‡Œæš‚æ—¶çœç•¥å…¶ä»–å¤æ‚äº‹ä»¶å¤„ç†é€»è¾‘)

            # å¤„ç†ä¸Šä¸‹æ–‡é‡ç½®äº‹ä»¶ - æ ‡è®°ä½†ä¸ç«‹å³ç»“æŸ
            context_reset_pending_tasks = ""
            if "update_task_description" in event_data:
                try:
                    logger.info("[OpenAI Compat] æ£€æµ‹åˆ°ä¸Šä¸‹æ–‡é‡ç½®äº‹ä»¶ï¼Œå‡†å¤‡ä»»åŠ¡å»¶ç»­æç¤º")

                    # æå–ä»»åŠ¡æè¿°ä¸­çš„å¾…å¤„ç†ä»»åŠ¡
                    task_desc = event_data.get("update_task_description", {}).get("description", "")

                    # ç®€å•è§£æå¾…å¤„ç†ä»»åŠ¡
                    if "Pending Tasks:" in task_desc:
                        tasks_section = task_desc.split("Pending Tasks:")[1].split("\n\n")[0]
                        context_reset_pending_tasks = f"\n\nğŸ“‹ **ä¸Šä¸‹æ–‡å·²é‡ç½®ï¼Œä½†æœ‰å¾…å¤„ç†ä»»åŠ¡:**\n{tasks_section.strip()}\n\nâš ï¸ **é‡è¦æé†’ï¼š** ä¸ºé¿å…é‡å¤é‡ç½®ï¼Œè¯·ï¼š\nâ€¢ æ–¹æ¡ˆ1ï¼šæ‰§è¡Œå‹ç¼©ä¸Šä¸‹æ–‡æŒ‡ä»¤ï¼ˆå¦‚ `/compact`ï¼‰\nâ€¢ æ–¹æ¡ˆ2ï¼šå¼€å¯æ–°å¯¹è¯ç»§ç»­æœªå®Œæˆçš„ä»»åŠ¡\n\nğŸ’¡ è¯·ç»§ç»­ä¹‹å‰çš„å·¥ä½œæˆ–è¯¢é—®éœ€è¦å®Œæˆçš„å…·ä½“ä»»åŠ¡ã€‚"
                    elif "Optional Next Step:" in task_desc:
                        next_step_section = task_desc.split("Optional Next Step:")[1].split("\n\n")[0]
                        context_reset_pending_tasks = f"\n\nğŸ“‹ **ä¸Šä¸‹æ–‡å·²é‡ç½®ï¼Œå»ºè®®ä¸‹ä¸€æ­¥:**\n{next_step_section.strip()}\n\nâš ï¸ **é‡è¦æé†’ï¼š** ä¸ºé¿å…é‡å¤é‡ç½®ï¼Œè¯·ï¼š\nâ€¢ æ–¹æ¡ˆ1ï¼šæ‰§è¡Œå‹ç¼©ä¸Šä¸‹æ–‡æŒ‡ä»¤ï¼ˆå¦‚ `/compact`ï¼‰\nâ€¢ æ–¹æ¡ˆ2ï¼šå¼€å¯æ–°å¯¹è¯ç»§ç»­æœªå®Œæˆçš„ä»»åŠ¡\n\nğŸ’¡ è¯·ç»§ç»­ä¹‹å‰çš„å·¥ä½œæˆ–è¯¢é—®éœ€è¦å®Œæˆçš„å…·ä½“ä»»åŠ¡ã€‚"
                    else:
                        context_reset_pending_tasks = f"\n\nğŸ“‹ **ä¸Šä¸‹æ–‡å·²é‡ç½®**\n\nâš ï¸ **é‡è¦æé†’ï¼š** ä¸ºé¿å…é‡å¤é‡ç½®ï¼Œè¯·ï¼š\nâ€¢ æ–¹æ¡ˆ1ï¼šæ‰§è¡Œå‹ç¼©ä¸Šä¸‹æ–‡æŒ‡ä»¤ï¼ˆå¦‚ `/compact`ï¼‰\nâ€¢ æ–¹æ¡ˆ2ï¼šå¼€å¯æ–°å¯¹è¯ç»§ç»­å·¥ä½œ\n\nğŸ’¡ å¯¹è¯ä¸Šä¸‹æ–‡è¿‡é•¿å·²è‡ªåŠ¨é‡ç½®ã€‚å¦‚æœ‰æœªå®Œæˆçš„ä»»åŠ¡ï¼Œè¯·é‡æ–°è¯´æ˜éœ€è¦ç»§ç»­çš„å·¥ä½œã€‚"

                    # å‘é€ä»»åŠ¡å»¶ç»­æç¤º
                    if context_reset_pending_tasks:
                        output_text += context_reset_pending_tasks  # ç´¯ç§¯è¾“å‡ºæ–‡æœ¬
                        continuation_delta = {
                            "id": completion_id,
                            "object": "chat.completion.chunk",
                            "created": created_ts,
                            "model": model_id,
                            "choices": [{"index": 0, "delta": {"content": context_reset_pending_tasks}}],
                        }
                        logger.info("[OpenAI Compat] å‘é€ä»»åŠ¡å»¶ç»­æç¤º: %s", json.dumps(continuation_delta, ensure_ascii=False))
                        yield f"data: {json.dumps(continuation_delta, ensure_ascii=False)}\n\n"

                except Exception as e:
                    logger.error(f"[OpenAI Compat] å¤„ç†ä¸Šä¸‹æ–‡é‡ç½®äº‹ä»¶å¤±è´¥: {e}")
                    # ç»§ç»­æ­£å¸¸æµç¨‹

            if "finished" in event_data:
                # Extract token usage from finished event
                finished_data = event_data.get("finished", {})
                request_cost = finished_data.get("request_cost", {})
                context_window_info = finished_data.get("context_window_info", {})

                # æ£€æµ‹åˆ° internal_error - æŠ›å‡ºå¯æ¢å¤å¼‚å¸¸ä»¥è§¦å‘è‡ªåŠ¨é‡è¯•
                if "internal_error" in finished_data:
                    error_info = finished_data["internal_error"]
                    error_message = error_info.get("message", "Unknown internal error")

                    logger.error(f"[OpenAI Compat] æœåŠ¡è¿”å› internal_error: {error_message}")

                    # å°è¯•ä»é”™è¯¯æ¶ˆæ¯ä¸­æå–å·¥å…·åç§°
                    tool_name = None
                    import re
                    tool_match = re.search(r'tool_call:\{[^}]*?(\w+):\{\}', error_message)
                    if tool_match:
                        tool_name = tool_match.group(1)

                    # æŠ›å‡ºå¯æ¢å¤å¼‚å¸¸ï¼Œç”±å¤–å±‚å¤„ç†è‡ªåŠ¨é‡è¯•
                    raise InternalErrorRecoverable(tool_name, error_message)

                # æ£€æµ‹åˆ° llm_unavailable - æŠ›å‡ºå¯æ¢å¤å¼‚å¸¸ä»¥è§¦å‘è‡ªåŠ¨é‡è¯•
                if "llm_unavailable" in finished_data:
                    logger.error("[OpenAI Compat] æœåŠ¡è¿”å› llm_unavailable")
                    raise LLMUnavailableRecoverable()

                # ä½¿ç”¨ tiktoken å‡†ç¡®è®¡ç®—è¾“å‡º token æ•°
                estimated_output_tokens = count_tokens(output_text, model_id) if output_text else 0
                if estimated_output_tokens == 0:
                    estimated_output_tokens = 1  # è‡³å°‘ 1 ä¸ª token

                # è®¡ç®—è¾“å…¥ tokenï¼šä½¿ç”¨ context_window_info çš„æ¯”ä¾‹å€¼
                # context_window_info åŒ…å«ä¸€ä¸ª 0-1 çš„æ¯”ä¾‹å€¼ï¼Œè¡¨ç¤ºä½¿ç”¨äº†å¤šå°‘ä¸Šä¸‹æ–‡çª—å£
                # ä¾‹å¦‚ 0.45 è¡¨ç¤ºä½¿ç”¨äº† 45% çš„ä¸Šä¸‹æ–‡çª—å£
                if context_window_info:
                    # ä» context_window_info ä¸­è·å–ä½¿ç”¨æ¯”ä¾‹
                    # context_window_info å¯èƒ½æ˜¯ä¸€ä¸ªå­—å…¸æˆ–ç›´æ¥æ˜¯æ•°å€¼
                    if isinstance(context_window_info, dict):
                        # å¦‚æœæ˜¯å­—å…¸ï¼ŒæŸ¥æ‰¾å¯èƒ½çš„é”®
                        context_usage = context_window_info.get("context_window_usage", 0) or context_window_info.get("used", 0) or context_window_info.get("ratio", 0)
                    else:
                        # å¦‚æœç›´æ¥æ˜¯æ•°å€¼
                        context_usage = float(context_window_info) if context_window_info else 0

                    # è·å–æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—å£å¤§å°
                    max_context = get_model_context_window(model_id)

                    # è®¡ç®—å®é™…ä½¿ç”¨çš„ token æ•°
                    estimated_input_tokens = int(context_usage * max_context) if context_usage > 0 else (input_tokens if input_tokens > 0 else 1000)

                    # è®°å½•æ—¥å¿—
                    logger.info(f"[OpenAI Compat] Token è®¡ç®—: context_usage={context_usage}, max_context={max_context}, prompt_tokens={estimated_input_tokens}")
                else:
                    # å¦‚æœæ²¡æœ‰ context_window_infoï¼Œä½¿ç”¨ä¼ å…¥çš„é¢„è®¡ç®—å€¼æˆ–é»˜è®¤å€¼
                    estimated_input_tokens = input_tokens if input_tokens > 0 else 1000

                done_chunk = {
                    "id": completion_id,
                    "object": "chat.completion.chunk",
                    "created": created_ts,
                    "model": model_id,
                    "choices": [{"index": 0, "delta": {}, "finish_reason": ("tool_calls" if tool_calls_emitted else "stop")}],
                    "usage": {
                        "prompt_tokens": estimated_input_tokens,
                        "completion_tokens": estimated_output_tokens,
                        "total_tokens": estimated_input_tokens + estimated_output_tokens
                    }
                }
                try:
                    logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit done): %s", json.dumps(done_chunk, ensure_ascii=False))
                except Exception:
                    pass
                yield f"data: {json.dumps(done_chunk, ensure_ascii=False)}\n\n"

    # æ‰“å°å®Œæˆæ ‡è®°
    try:
        logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit): [DONE]")
    except Exception:
        pass
    yield "data: [DONE]\n\n"


def estimate_input_tokens(packet: Dict[str, Any]) -> int:
    """ä¼°ç®—è¾“å…¥ packet ä¸­çš„ token æ•°é‡

    Args:
        packet: åŒ…å«è¯·æ±‚æ•°æ®çš„å­—å…¸

    Returns:
        ä¼°ç®—çš„ token æ•°é‡
    """
    total_chars = 0

    # è®¡ç®— user inputs ä¸­çš„æ–‡æœ¬
    if "input" in packet and "user_inputs" in packet["input"]:
        inputs = packet["input"]["user_inputs"].get("inputs", [])
        for inp in inputs:
            if isinstance(inp, dict):
                # å¤„ç† text å­—æ®µ
                if "text" in inp:
                    total_chars += len(str(inp["text"]))
                # å¤„ç† attachments ä¸­çš„æ–‡æœ¬
                if "attachments" in inp:
                    for attachment in inp["attachments"]:
                        if isinstance(attachment, dict) and "text" in attachment:
                            total_chars += len(str(attachment["text"]))

                # å¤„ç† user_query ä¸­çš„å†…å®¹ï¼ˆåŒ…æ‹¬æŸ¥è¯¢å’Œå¼•ç”¨é™„ä»¶ï¼‰
                if "user_query" in inp:
                    user_query = inp["user_query"]
                    if isinstance(user_query, dict):
                        # è®¡ç®—æŸ¥è¯¢æ–‡æœ¬
                        if "query" in user_query:
                            total_chars += len(str(user_query["query"]))

                        # é‡è¦ï¼šè®¡ç®— referenced_attachmentsï¼ˆç³»ç»Ÿæç¤ºè¯ã€å·¥å…·é™åˆ¶ç­‰ï¼‰
                        if "referenced_attachments" in user_query:
                            refs = user_query["referenced_attachments"]
                            if isinstance(refs, dict):
                                for key, ref in refs.items():
                                    if isinstance(ref, dict):
                                        # å¤„ç†çº¯æ–‡æœ¬é™„ä»¶
                                        if "plain_text" in ref:
                                            total_chars += len(str(ref["plain_text"]))
                                        # å¤„ç†å…¶ä»–å¯èƒ½çš„æ–‡æœ¬å­—æ®µ
                                        if "text" in ref:
                                            total_chars += len(str(ref["text"]))

    # è®¡ç®— task_context ä¸­çš„å†å²æ¶ˆæ¯
    if "task_context" in packet:
        # messages å¯èƒ½åœ¨ task_context.messages æˆ– task_context.tasks[0].messages
        messages = []

        # å°è¯•ç›´æ¥ä» task_context è·å– messages
        if "messages" in packet["task_context"]:
            messages = packet["task_context"]["messages"]
        # å°è¯•ä» tasks åˆ—è¡¨è·å– messages
        elif "tasks" in packet["task_context"] and packet["task_context"]["tasks"]:
            for task in packet["task_context"]["tasks"]:
                if isinstance(task, dict) and "messages" in task:
                    messages.extend(task["messages"])

        for msg in messages:
            if isinstance(msg, dict):
                # å¤„ç† agent_output
                if "agent_output" in msg:
                    output = msg["agent_output"]
                    if isinstance(output, dict) and "text" in output:
                        total_chars += len(str(output["text"]))
                # å¤„ç† user_input
                if "user_input" in msg:
                    user_input = msg["user_input"]
                    if isinstance(user_input, dict) and "text" in user_input:
                        total_chars += len(str(user_input["text"]))

    # è®¡ç®—å·¥å…·å®šä¹‰çš„å­—ç¬¦æ•°
    if "mcp_context" in packet and "tools" in packet["mcp_context"]:
        tools = packet["mcp_context"]["tools"]
        # å·¥å…·å®šä¹‰é€šå¸¸æ¯”è¾ƒé•¿ï¼Œç®€å•ä¼°ç®—
        total_chars += len(json.dumps(tools, ensure_ascii=False))

    # ä¼°ç®— token æ•°ï¼šå¹³å‡æ¯ä¸ª token çº¦ 4 ä¸ªå­—ç¬¦ï¼ˆè‹±æ–‡ï¼‰
    # å¯¹äºæ··åˆä¸­è‹±æ–‡å†…å®¹ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆç†çš„ä¼°ç®—
    estimated_tokens = max(total_chars // 4, 1)

    return estimated_tokens


async def stream_openai_sse(
    packet: Dict[str, Any],
    completion_id: str,
    created_ts: int,
    model_id: str,
    retry_count: int = 0,
    restricted_tools: Optional[Set[str]] = None
) -> AsyncGenerator[str, None]:
    try:
        # ä½¿ç”¨æ–°çš„ tiktoken è®¡ç®—è¾“å…¥ token æ•°
        input_tokens = count_packet_tokens(packet, model_id)
        logger.info(f"[OpenAI Compat] è®¡ç®—çš„è¾“å…¥ token æ•° (tiktoken): {input_tokens}")

        # ä»…åœ¨é¦–æ¬¡è°ƒç”¨æ—¶å‘é€ role é¦–å—ï¼Œé¿å…é‡è¯•æ—¶é‡å¤å‘é€
        if retry_count == 0:
            first = {
                "id": completion_id,
                "object": "chat.completion.chunk",
                "created": created_ts,
                "model": model_id,
                "choices": [{"index": 0, "delta": {"role": "assistant"}}],
            }
            # æ‰“å°è½¬æ¢åçš„é¦–ä¸ª SSE äº‹ä»¶ï¼ˆOpenAI æ ¼å¼ï¼‰
            try:
                logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit): %s", json.dumps(first, ensure_ascii=False))
            except Exception:
                pass
            yield f"data: {json.dumps(first, ensure_ascii=False)}\n\n"

        timeout = httpx.Timeout(600.0)
        async with httpx.AsyncClient(http2=True, timeout=timeout, trust_env=True) as client:
            def _do_stream():
                """åˆ›å»ºä¸€ä¸ªæ–°çš„æµå¼è¯·æ±‚"""
                return client.stream(
                    "POST",
                    f"{BRIDGE_BASE_URL}/api/warp/send_stream_sse",
                    headers={"accept": "text/event-stream"},
                    json={"json_data": packet, "message_type": "warp.multi_agent.v1.Request"},
                )

            async def _check_response_error(response):
                """æ£€æŸ¥å“åº”çŠ¶æ€ç ï¼Œå¦‚æœä¸æ˜¯200åˆ™æŠ›å‡ºé”™è¯¯"""
                if response.status_code != 200:
                    error_text = await response.aread()
                    error_content = error_text.decode("utf-8") if error_text else ""
                    logger.error(f"[OpenAI Compat] Bridge HTTP error {response.status_code}: {error_content[:300]}")
                    raise RuntimeError(f"bridge error: {error_content}")

            # é¦–æ¬¡è¯·æ±‚
            response_cm = _do_stream()
            async with response_cm as response:
                # å¤„ç† 429 é”™è¯¯ï¼ˆä»¤ç‰Œè¿‡æœŸï¼‰
                if response.status_code == 429:
                    try:
                        r = await client.post(f"{BRIDGE_BASE_URL}/api/auth/refresh", timeout=10.0)
                        logger.warning("[OpenAI Compat] Bridge returned 429. Tried JWT refresh -> HTTP %s", r.status_code)
                    except Exception as _e:
                        logger.warning("[OpenAI Compat] JWT refresh attempt failed after 429: %s", _e)
                    # é‡è¯•è¯·æ±‚
                    response_cm2 = _do_stream()
                    async with response_cm2 as response2:
                        await _check_response_error(response2)
                        # ä½¿ç”¨å…±ç”¨å‡½æ•°å¤„ç†å“åº”æµ
                        async for chunk in _process_sse_response_lines(response2, completion_id, created_ts, model_id, input_tokens):
                            yield chunk
                    return

                # æ£€æŸ¥å“åº”çŠ¶æ€ç 
                await _check_response_error(response)

                # ä½¿ç”¨å…±ç”¨å‡½æ•°å¤„ç†å“åº”æµ
                async for chunk in _process_sse_response_lines(response, completion_id, created_ts, model_id, input_tokens):
                    yield chunk
    except InternalErrorRecoverable as e:
        # æ£€æµ‹åˆ°å¯æ¢å¤çš„ internal_errorï¼Œå°è¯•è‡ªåŠ¨é‡è¯•
        if retry_count >= 1:
            # å·²ç»é‡è¯•è¿‡ä¸€æ¬¡ï¼Œä¸å†é‡è¯•
            logger.error(f"[OpenAI Compat] Internal error æ¢å¤å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°: {e.tool_name}")
            error_text = (
                f"\n\nâš ï¸ **æœåŠ¡å†…éƒ¨é”™è¯¯ï¼ˆæ— æ³•è‡ªåŠ¨æ¢å¤ï¼‰**\n\n"
                f"AI å¤šæ¬¡å°è¯•è°ƒç”¨è¢«é™åˆ¶çš„å·¥å…·ï¼š`{e.tool_name}`\n\n"
                f"**å»ºè®®è§£å†³æ–¹æ¡ˆï¼š**\n"
                f"â€¢ ğŸ”„ æ¢ä¸ªæ–¹å¼æè¿°ä½ çš„éœ€æ±‚\n"
                f"â€¢ ğŸ’¡ ç®€åŒ–è¯·æ±‚èŒƒå›´\n"
                f"â€¢ ğŸ“ æ˜ç¡®è¯´æ˜é¿å…æŸäº›æ“ä½œ\n"
            )
            error_delta = {
                "id": completion_id,
                "object": "chat.completion.chunk",
                "created": created_ts,
                "model": model_id,
                "choices": [{"index": 0, "delta": {"content": error_text}}],
            }
            yield f"data: {json.dumps(error_delta, ensure_ascii=False)}\n\n"

            done_chunk = {
                "id": completion_id,
                "object": "chat.completion.chunk",
                "created": created_ts,
                "model": model_id,
                "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": input_tokens, "completion_tokens": 50, "total_tokens": input_tokens + 50}
            }
            yield f"data: {json.dumps(done_chunk, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
            return

        # ç¬¬ä¸€æ¬¡é‡åˆ°é”™è¯¯ï¼Œè‡ªåŠ¨æ¢å¤
        logger.info(f"[OpenAI Compat] æ£€æµ‹åˆ° internal_errorï¼Œè‡ªåŠ¨æ¢å¤ä¸­... (å·¥å…·: {e.tool_name}, é‡è¯•æ¬¡æ•°: {retry_count})")

        # åˆå§‹åŒ– restricted_tools é›†åˆ
        if restricted_tools is None:
            restricted_tools = set()

        # è®°å½•å¤±è´¥çš„å·¥å…·
        if e.tool_name:
            restricted_tools.add(e.tool_name)

        # æ„é€ æ¢å¤æç¤º
        if e.tool_name:
            recovery_prompt = f"\n\n[ç³»ç»Ÿè‡ªåŠ¨æ¢å¤] è¯·ç»§ç»­ä¹‹å‰çš„ä»»åŠ¡ï¼Œä½†ä¸è¦ä½¿ç”¨ {e.tool_name} å·¥å…·ã€‚å¯ç”¨çš„å·¥å…·åŒ…æ‹¬ï¼šReadã€Writeã€Editã€Bashã€Globã€Grep ç­‰ MCP å·¥å…·ã€‚"
        else:
            recovery_prompt = "\n\n[ç³»ç»Ÿè‡ªåŠ¨æ¢å¤] è¯·ç»§ç»­ä¹‹å‰çš„ä»»åŠ¡ï¼Œä½¿ç”¨å¯ç”¨çš„ MCP å·¥å…·å®Œæˆã€‚"

        # å‘é€æ¢å¤æç¤ºç»™ç”¨æˆ·ï¼ˆè®©ç”¨æˆ·çŸ¥é“æ­£åœ¨è‡ªåŠ¨æ¢å¤ï¼‰
        recovery_notice = f"\n\nğŸ”„ **æ­£åœ¨è‡ªåŠ¨æ¢å¤...**\n\næ£€æµ‹åˆ°å·¥å…·é™åˆ¶å†²çªï¼Œç³»ç»Ÿæ­£åœ¨é‡æ–°å°è¯•ä»»åŠ¡ã€‚\n"
        notice_delta = {
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created_ts,
            "model": model_id,
            "choices": [{"index": 0, "delta": {"content": recovery_notice}}],
        }
        yield f"data: {json.dumps(notice_delta, ensure_ascii=False)}\n\n"

        # ä¿®æ”¹ packetï¼Œåœ¨ç”¨æˆ·æŸ¥è¯¢ä¸­æ·»åŠ æ¢å¤æç¤º
        import copy
        new_packet = copy.deepcopy(packet)

        # åœ¨ user_inputs ä¸­çš„æœ€åä¸€ä¸ª user_query æ·»åŠ æ¢å¤æç¤º
        if "input" in new_packet and "user_inputs" in new_packet["input"]:
            inputs = new_packet["input"]["user_inputs"].get("inputs", [])
            if inputs:
                last_input = inputs[-1]
                if "user_query" in last_input and isinstance(last_input["user_query"], dict):
                    # åœ¨æŸ¥è¯¢æœ«å°¾é™„åŠ æ¢å¤æç¤º
                    current_query = last_input["user_query"].get("query", "")
                    # é¿å…é‡å¤é™„åŠ ç›¸åŒæ¢å¤æç¤º
                    if "[ç³»ç»Ÿè‡ªåŠ¨æ¢å¤]" not in current_query:
                        last_input["user_query"]["query"] = current_query + recovery_prompt
                        logger.info(f"[OpenAI Compat] å·²åœ¨è¯·æ±‚ä¸­æ·»åŠ æ¢å¤æç¤º: {recovery_prompt[:100]}...")
                    else:
                        logger.info("[OpenAI Compat] æ£€æµ‹åˆ°å·²åŒ…å«ç³»ç»Ÿè‡ªåŠ¨æ¢å¤æç¤ºï¼Œè·³è¿‡è¿½åŠ ")

        # é€’å½’è°ƒç”¨è‡ªå·±ï¼Œä½¿ç”¨æ–°çš„ packet å’Œå¢åŠ çš„ retry_count
        logger.info(f"[OpenAI Compat] å¼€å§‹è‡ªåŠ¨é‡è¯• (retry_count={retry_count + 1})")
        async for chunk in stream_openai_sse(new_packet, completion_id, created_ts, model_id, retry_count + 1, restricted_tools):
            yield chunk

    except LLMUnavailableRecoverable:
        # æ£€æµ‹åˆ° llm_unavailableï¼Œå°è¯•è‡ªåŠ¨é‡è¯•
        if retry_count >= 1:
            logger.error("[OpenAI Compat] LLM unavailable æ¢å¤å¤±è´¥ï¼Œå·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
            error_text = "\n\nâš ï¸ **LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨**\n\nè¯·ç¨åé‡è¯•ã€‚\n"
            error_delta = {
                "id": completion_id,
                "object": "chat.completion.chunk",
                "created": created_ts,
                "model": model_id,
                "choices": [{"index": 0, "delta": {"content": error_text}}],
            }
            yield f"data: {json.dumps(error_delta, ensure_ascii=False)}\n\n"

            done_chunk = {
                "id": completion_id,
                "object": "chat.completion.chunk",
                "created": created_ts,
                "model": model_id,
                "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}],
                "usage": {"prompt_tokens": input_tokens, "completion_tokens": 10, "total_tokens": input_tokens + 10}
            }
            yield f"data: {json.dumps(done_chunk, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
            return

        # ç¬¬ä¸€æ¬¡é‡åˆ° llm_unavailableï¼Œè‡ªåŠ¨æ¢å¤
        logger.info(f"[OpenAI Compat] æ£€æµ‹åˆ° llm_unavailableï¼Œè‡ªåŠ¨æ¢å¤ä¸­... (é‡è¯•æ¬¡æ•°: {retry_count})")

        # å‘é€æ¢å¤æç¤ºç»™ç”¨æˆ·
        recovery_notice = "\n\nğŸ”„ **LLM æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œæ­£åœ¨è‡ªåŠ¨é‡è¯•...**\n\n"
        notice_delta = {
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created_ts,
            "model": model_id,
            "choices": [{"index": 0, "delta": {"content": recovery_notice}}],
        }
        yield f"data: {json.dumps(notice_delta, ensure_ascii=False)}\n\n"

        # ä¿®æ”¹ packetï¼Œæ·»åŠ ç»§ç»­ä»»åŠ¡æç¤º
        import copy
        new_packet = copy.deepcopy(packet)

        if "input" in new_packet and "user_inputs" in new_packet["input"]:
            inputs = new_packet["input"]["user_inputs"].get("inputs", [])
            if inputs:
                last_input = inputs[-1]
                if "user_query" in last_input and isinstance(last_input["user_query"], dict):
                    current_query = last_input["user_query"].get("query", "")
                    if "ç»§ç»­ä»»åŠ¡" not in current_query and "[è‡ªåŠ¨æ¢å¤]" not in current_query:
                        recovery_prompt = "\n\n[è‡ªåŠ¨æ¢å¤] ç»§ç»­ä¹‹å‰çš„ä»»åŠ¡ã€‚"
                        last_input["user_query"]["query"] = current_query + recovery_prompt
                        logger.info("[OpenAI Compat] å·²åœ¨è¯·æ±‚ä¸­æ·»åŠ ç»§ç»­ä»»åŠ¡æç¤º")

        # é€’å½’è°ƒç”¨ï¼Œé‡è¯•
        logger.info(f"[OpenAI Compat] å¼€å§‹è‡ªåŠ¨é‡è¯• llm_unavailable (retry_count={retry_count + 1})")
        async for chunk in stream_openai_sse(new_packet, completion_id, created_ts, model_id, retry_count + 1, restricted_tools):
            yield chunk

    except Exception as e:
        import traceback
        error_msg = str(e) if str(e) else repr(e)
        logger.error(f"[OpenAI Compat] stream processing failed: {error_msg}")
        logger.error(f"[OpenAI Compat] Exception type: {type(e).__name__}")
        logger.error(f"[OpenAI Compat] Traceback: {traceback.format_exc()}")
        error_chunk = {
            "id": completion_id,
            "object": "chat.completion.chunk",
            "created": created_ts,
            "model": model_id,
            "choices": [{"index": 0, "delta": {}, "finish_reason": "error"}],
            "error": {"message": error_msg},
        }
        try:
            logger.info("[OpenAI Compat] è½¬æ¢åçš„ SSE(emit error): %s", json.dumps(error_chunk, ensure_ascii=False))
        except Exception:
            pass
        yield f"data: {json.dumps(error_chunk, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n" 